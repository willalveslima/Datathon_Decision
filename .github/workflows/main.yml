# .github/workflows/main.yml

name: CI/CD Pipeline

# Gatilhos para o pipeline
on:
  push:
    branches:
      - main
      - develop
      - feature/* # Para branches de feature
  pull_request:
    branches:
      - main
      - develop

jobs:
  build-and-test:
    runs-on: ubuntu-latest # Ambiente de execução da Action (pode ser ajustado)

    steps:
      - name: Checkout code
        uses: actions/checkout@v4 # Clona o repositório

      - name: Authenticate to Google Cloud # NOVO PASSO: Autentica com a GCP
        uses: "google-github-actions/auth@v2"
        with:
          # A chave JSON da sua Service Account da GCP, armazenada como um GitHub Secret
          # Ex: GCP_SA_KEY
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v1

      - name: Use gcloud
        run: gcloud config set project ${{ secrets.GCP_PROJECT_ID }}

      - name: Verify GCP Authentication (DEBUG)
        run: |
          gcloud auth list
          gcloud config list project
          gsutil ls gs://datathon-decision/data/raw/

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11" # Garante o uso do Python 3.11
          cache: "pip" # Habilita cache para dependências pip

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Instala todas as dependências do projeto (incluindo dev/test)
          pip install -r requirements.txt
          # Instala ferramentas de cobertura e análise estática que não estejam no requirements.txt principal
          pip install pytest-cov black isort pydocstyle pylint
          # Instala gcloud CLI para interagir com o Cloud Storage
          # Já vem pré-instalado na maioria dos runners do GitHub Actions, mas pode ser instalado explicitamente se necessário.
          # pip install google-cloud-sdk # Não é necessário se usar 'google-github-actions/setup-gcloud' ou já estiver no runner

      - name: Download Raw Data from GCS # ATUALIZADO: Baixa os dados brutos do Cloud Storage
        run: |
          echo "Downloading raw data from GCS bucket: datathon-decision..."
          mkdir -p data/raw # Garante que a pasta de destino exista
          # Adapte os nomes dos arquivos se forem diferentes no seu bucket
          gsutil cp gs://datathon-decision/data/raw/applicants.json data/raw/applicants.json
          gsutil cp gs://datathon-decision/data/raw/vagas.json data/raw/vagas.json
          gsutil cp gs://datathon-decision/data/raw/prospects.json data/raw/prospects.json
          echo "Raw data downloaded."

      - name: Run Data Preprocessing
        # Executa o script de pré-processamento para gerar o merged_data_processed.csv
        # e garantir que os dados estejam prontos para o treinamento.
        run: |
          python src/training/data_preprocessing_merge_code.py

      - name: Run Model Training
        # Executa o script de treinamento para gerar os artefatos do modelo (.pkl, .keras, .txt)
        # na pasta 'models/'. Isso é crucial, pois 'models/' não está no Git.
        run: |
          python src/training/model_training.py

      - name: Upload Model Artifacts to GCS # NOVO PASSO: Faz upload dos artefatos do modelo para o Cloud Storage
        run: |
          echo "Uploading model artifacts to GCS bucket: datathon-decision..."
          # Copia todos os arquivos da pasta 'models/' para o subdiretório 'models/' no bucket
          gsutil cp -r models/* gs://datathon-decision/models/
          echo "Model artifacts uploaded."

      - name: Run Tests with Coverage
        # Executa os testes unitários e mede a cobertura de código.
        # PYTHONPATH=. é necessário para que pytest encontre os módulos src/
        run: |
          PYTHONPATH=. pytest --cov=src --cov-fail-under=80
      
      

      - name: Build Docker Images
        # Constrói as imagens Docker para a API e o serviço de drift.
        # A pasta 'models/' agora existe e contém os artefatos gerados na etapa de treinamento.
        # NOTA: Os Dockerfiles precisarão ser atualizados para BAIXAR os modelos do GCS,
        # em vez de copiá-los da pasta local 'models/'.
        run: |
          
          echo "Building API Docker image..."
          docker build -t recommendation-api:$(git rev-parse --short HEAD) -f docker/api/Dockerfile .
          
          echo "Building Drift Monitor Docker image..."
          docker build -t drift-monitor:$(git rev-parse --short HEAD) -f docker/drift_monitor/Dockerfile . 

      - name: Push Docker Images to Artifact Registry # NOVO PASSO: Envia as imagens para o Artifact Registry da GCP
        # Requer que a Service Account usada na autenticação tenha permissão de Artifact Registry Writer
        run: |
          # Configura o Docker para usar o Artifact Registry
          gcloud auth configure-docker southamerica-east1-docker.pkg.dev
          
          # Define a tag completa da imagem para o Artifact Registry
          API_IMAGE_TAG=southamerica-east1-docker.pkg.dev/${{ secrets.GCP_PROJECT_ID }}/fiap-datathon/recommendation-api:$(git rev-parse --short HEAD)
          DRIFT_IMAGE_TAG=southamerica-east1-docker.pkg.dev/${{ secrets.GCP_PROJECT_ID }}/fiap-datathon/drift-monitor:$(git rev-parse --short HEAD)
          
          # Tagueia e envia as imagens
          docker tag recommendation-api:$(git rev-parse --short HEAD) $API_IMAGE_TAG
          docker push $API_IMAGE_TAG
          
          docker tag drift-monitor:$(git rev-parse --short HEAD) $DRIFT_IMAGE_TAG
          docker push $DRIFT_IMAGE_TAG
          
          echo "Docker images pushed to Artifact Registry."

      - name: Get Cloud SQL Connection Name # NOVO PASSO: Obtém o nome de conexão do Cloud SQL
        id: get_cloudsql_conn_name
        run: |
          # Substitua <sua-instancia-cloudsql> pelo nome da sua instância Cloud SQL
          # Ex: ml-reco-db-fiap-datathon-decision-466317
          SQL_CONN_NAME=$(gcloud sql instances describe ml-reco-db-fiap-datathon-decision --format='value(connectionName)')
          echo "cloud_sql_connection_name=$SQL_CONN_NAME" >> $GITHUB_OUTPUT
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }} # Use o secret para o ID do projeto

      - name: Set short SHA
        run: echo "SHORT_SHA=$(git rev-parse --short ${{ github.sha }})" >> $GITHUB_ENV

      - name: Deploy Prediction API to Cloud Run # NOVO PASSO: Implanta a API de Predição no Cloud Run
        uses: 'google-github-actions/deploy-cloudrun@v2'
        with:
          service: prediction-api-service # Nome do serviço Cloud Run
          image: southamerica-east1-docker.pkg.dev/${{ secrets.GCP_PROJECT_ID }}/fiap-datathon/recommendation-api:${{SHORT_SHA}}
          region: southamerica-east1 # Sua região do Cloud Run
          # Passa as variáveis de ambiente do DB para o serviço Cloud Run
          env_vars: |
            DB_NAME=${{ secrets.DB_NAME_SECRET }}
            DB_USER=${{ secrets.DB_USER_SECRET }}
            DB_PASSWORD=${{ secrets.DB_PASSWORD_SECRET }}
            DB_HOST=/cloudsql/${{ steps.get_cloudsql_conn_name.outputs.cloud_sql_connection_name }} # Conexão via Cloud SQL Proxy
            DB_PORT=5432
          # Configura a Service Account para o Cloud Run (deve ter permissão de Cloud SQL Client)
          service_account_email: cloud-run-sa-${{ secrets.GCP_PROJECT_ID }}@${{ secrets.GCP_PROJECT_ID }}.iam.gserviceaccount.com
          # Permite acesso não autenticado para teste (REMOVA EM PRODUÇÃO)
          allow_unauthenticated: true

      - name: Deploy Drift Monitor Service to Cloud Run # NOVO PASSO: Implanta o Serviço de Drift no Cloud Run
        uses: 'google-github-actions/deploy-cloudrun@v2'
        with:
          service: drift-monitor-service # Nome do serviço Cloud Run
          image: southamerica-east1-docker.pkg.dev/${{ secrets.GCP_PROJECT_ID }}/fiap-datathon/drift-monitor:${{SHORT_SHA}}
          region: southamerica-east1 # Sua região do Cloud Run
          # Passa as variáveis de ambiente do DB para o serviço Cloud Run
          env_vars: |
            DB_NAME=${{ secrets.DB_NAME_SECRET }}
            DB_USER=${{ secrets.DB_USER_SECRET }}
            DB_PASSWORD=${{ secrets.DB_PASSWORD_SECRET }}
            DB_HOST=/cloudsql/${{ steps.get_cloudsql_conn_name.outputs.cloud_sql_connection_name }} # Conexão via Cloud SQL Proxy
            DB_PORT=5432
          service_account_email: cloud-run-sa-${{ secrets.GCP_PROJECT_ID }}@${{ secrets.GCP_PROJECT_ID }}.iam.gserviceaccount.com
          allow_unauthenticated: true # Permite acesso não autenticado para teste (REMOVA EM PRODUÇÃO)
